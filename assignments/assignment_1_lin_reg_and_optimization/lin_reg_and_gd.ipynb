{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks to https://github.com/esokolov for materials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "In this task you will develop linear regression and several methods of gradient descend\n",
    "\n",
    "\n",
    "### Grading\n",
    "Maximum grade is 10\n",
    "\n",
    "Cheating is bad, you should perform task by yourself\n",
    "\n",
    "Ineffective code implementation can negatively affect the score.\n",
    "Also, the score can be lowered for poorly readable code and poorly readable diagrams.\n",
    "\n",
    "All replies must be accompanied by code or comments on how they were received."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework assignment, you will implement different variations of gradient descent, write your own implementation of linear regression, compare gradient descent methods with each other on real data, and figure out how to select hyperparameters for these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Implementation of gradient descent (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will write your own implementations of various approaches to gradient descent based on the prepared templates in the file  `utils.py`:\n",
    "\n",
    "**Task 1.1. (1 point)**  **GradientDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "**Task 1.2. (1 point)**  **StochasticDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$ \n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\,$ - this is an estimate of the gradient from a batch of randomly selected objects.\n",
    "\n",
    "**Task 1.3. (1 point)**  **MomentumDescent**:\n",
    "\n",
    "$$\n",
    "    h_0 = 0, \\\\\n",
    "    h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "**Task 1.4. (1 point)** Adaptive gradient algorithm **Adagrad**:\n",
    "\n",
    "$$\n",
    "    G_0 = 0, \\\\\n",
    "    G_{k + 1} = G_{k} + \\left(\\nabla_{w} Q(w_{k})\\right) ^ 2, \\\\\n",
    "    w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\varepsilon + G_{k + 1}}} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "\n",
    "In all of the above methods, we will use the following formula for stride length:\n",
    "\n",
    "$$\n",
    "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
    "$$\n",
    "In practice, it is enough to set the parameter $\\lambda$, and for the rest, set the default parameters: $s_0 = 1, \\, p = 0.5.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MSE loss function:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "All calculations should be vectorised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Implementing Linear Regression (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will write your own implementation of linear regression trained using gradient descent based on the prepared templates in the file `utils.py` - **LinearRegression**.\n",
    "\n",
    "The following conditions must be met:\n",
    "\n",
    "* All calculations must be vectorized.\n",
    "* Loops in python are only allowed for gradient descent iterations.\n",
    "* As a stopping criterion, it is necessary to use (simultaneously):\n",
    "    * The square of the Euclidean norm of the difference of weights at two adjacent iterations is less than `tolerance`.\n",
    "    * Reaching the maximum number of iterations `max_iter`.\n",
    "* To track the convergence of the optimization process, we will use `loss_history`, in it we will store the values ​​of the loss function up to each step, starting from zero (up to the first step along the antigradient).\n",
    "* You need to initialize the weights with a zero vector or from the normal $ \\ mathcal {N} (0, 1) $ distribution (then you need to fix the seed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Checking the code (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import (\n",
    "    Adagrad,\n",
    "    GradientDescent,\n",
    "    MomentumDescent,\n",
    "    StochasticDescent,\n",
    ")\n",
    "from utils import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "X = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)\n",
    "\n",
    "lambda_ = 1e-2\n",
    "w0 = np.zeros(dimension)\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientDescent\n",
    "\n",
    "descent = GradientDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StochasticDescent\n",
    "\n",
    "descent = StochasticDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MomentumDescent\n",
    "\n",
    "descent = MomentumDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "\n",
    "descent = Adagrad(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression\n",
    "\n",
    "regression = LinearRegression(\n",
    "    descent = StochasticDescent(lambda_ = lambda_, w0 = w0, batch_size = 2),\n",
    "    tolerance = tolerance,\n",
    "    max_iter = max_iter\n",
    ")\n",
    "\n",
    "regression.fit(X, y)\n",
    "\n",
    "assert len(regression.loss_history) == max_iter, 'Loss history failed'\n",
    "\n",
    "prediction = regression.predict(X)\n",
    "\n",
    "assert prediction.shape[0] == num_objects, 'Predict failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Working with data (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available in file 'autos.csv'\n",
    "\n",
    "We will use a dataset of car dealerships on German Ebay. The price is our target variable.\n",
    "For further work, do the following:\n",
    "* Conduct reasonable data preprocessing.\n",
    "* Replace the target variable with its logarithm.\n",
    "* Divide the data into training, validation and test samples in a ratio of 3: 1: 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autos.csv  lin_reg_and_gd.ipynb  utils.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Comparison of gradient descent methods (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will compare gradient descent methods on the data you prepared from the previous task.\n",
    "\n",
    "* ** Task 5.1. (1.5 points) ** Choose the best $ \\ lambda $ step length for each method using the validation set. To do this, you can iterate over the logarithmic grid, since we are more interested in the order of magnitude than its exact value. Compare the quality of the methods for the MSE metric on the training and test samples, compare the number of iterations until convergence. All parameters except $ \\ lambda $ should be set equal to their default values.\n",
    "\n",
    "* ** Task 5.2. (0.5 points) ** Plot the dependence of the error function value on the iteration number (all methods are on one graph).\n",
    "\n",
    "Take a look at the results. Compare the methods with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.zeros(x_train.shape[1])\n",
    "\n",
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Convergence of stochastic gradient descent depending on the size of the batch (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will investigate the effect of batch size on stochastic gradient descent.\n",
    "\n",
    "* Make several runs (for example, k) of stochastic gradient descent on the training set for each batch size from the list. Measure the time and number of iterations until convergence. Calculate the mean and variance of these values ​​for each batch size.\n",
    "* Plot the dependence of the number of steps to convergence on the size of the batch.\n",
    "* Plot the time to convergence versus batch size.\n",
    "\n",
    "Take a look at the results. What conclusions can be drawn about the selection of the batch size for stochastic gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = np.arange(5, 500, 10)\n",
    "\n",
    "# YOUR CODE:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
